{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Edoka/Stevens/blob/main/Programming3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-Rx4Tw6p_b7"
      },
      "source": [
        "# Programming Assignment: Build a seq2seq model for machine translation.\n",
        "\n",
        "### Name: [Chimezie Edoka]\n",
        "\n",
        "### Task: Translate [English] to [spanish]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_RK6QBnFba6"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "xfA4EemVKsr1"
      },
      "outputs": [],
      "source": [
        "# !nvidia-smi -L  \n",
        "def reset_tensorflow_keras_backend():\n",
        "    # to be further investigated, but this seems to be enough\n",
        "    import tensorflow as tf\n",
        "    import keras as keras\n",
        "    tf.keras.backend.clear_session()\n",
        "    tf.compat.v1.reset_default_graph()\n",
        "    # _ = gc.collect()\n",
        "\n",
        "reset_tensorflow_keras_backend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVmtOvUJp_b9"
      },
      "source": [
        "## 0. You will do the following:\n",
        "\n",
        "1. Read and run my code.\n",
        "\n",
        "2. Complete the code in Section 1.1 and Section 4.2.\n",
        "\n",
        "    * Translation from **English** to **German** is not acceptable! Try another pair of languages.\n",
        "\n",
        "\n",
        "3. Make improvement: Using Bi-LSTM instead of LSTM.\n",
        "    \n",
        "4. Convert the .IPYNB file to .HTML file.\n",
        "\n",
        "    * The HTML file must contain the code and the output after execution.\n",
        "    \n",
        "    * Missing **the output after execution** will not be graded.\n",
        "\n",
        "\n",
        "5. Upload the .HTML file to your Google Drive, Dropbox, or Github repo. (If you submit the file to Google Drive or Dropbox, you must make the file \"open-access\". The delay caused by \"deny of access\" may result in late penalty.)\n",
        "\n",
        "6. On Canvas, submit the Google Drive/Dropbox/Github link to the HTML file.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbS54Er4p_b9"
      },
      "source": [
        "### Hint: \n",
        "\n",
        "To implement ```Bi-LSTM```, you will need the following code to build the encoder. Do NOT use Bi-LSTM for the decoder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cr7Vnwrkp_b9"
      },
      "outputs": [],
      "source": [
        "# from keras.layers import Bidirectional, Concatenate, LSTM\n",
        "\n",
        "# encoder_bilstm = Bidirectional(LSTM(latent_dim, return_state=True, \n",
        "#                                   dropout=0.5, name='encoder_lstm'))\n",
        "# _, forward_h, forward_c, backward_h, backward_c = encoder_bilstm(encoder_inputs)\n",
        "\n",
        "# state_h = Concatenate()([forward_h, backward_h])\n",
        "# state_c = Concatenate()([forward_c, backward_c])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4Vb6rM3p_b-"
      },
      "source": [
        "## 1. Data preparation\n",
        "\n",
        "1. Download data (e.g., \"deu-eng.zip\") from http://www.manythings.org/anki/\n",
        "2. Unzip the .ZIP file.\n",
        "3. Put the .TXT file (e.g., \"deu.txt\") in the directory \"./Data/\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUQX4rL0p_b-"
      },
      "source": [
        "### 1.1. Load and clean text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "dFvi-ElPp_b_"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import string\n",
        "from unicodedata import normalize\n",
        "import numpy\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "    # open the file as read only\n",
        "    file = open(filename, mode='rt', encoding='utf-8')\n",
        "    # read all text\n",
        "    text = file.read()\n",
        "    # close the file\n",
        "    file.close()\n",
        "    return text\n",
        "\n",
        "\n",
        "# split a loaded document into sentences\n",
        "def to_pairs(doc):\n",
        "    lines = doc.strip().split('\\n')\n",
        "    pairs = [line.split('\\t') for line in  lines]\n",
        "    return pairs\n",
        "\n",
        "def clean_data(lines):\n",
        "    cleaned = list()\n",
        "    # prepare regex for char filtering\n",
        "    re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
        "    # prepare translation table for removing punctuation\n",
        "    table = str.maketrans('', '', string.punctuation)\n",
        "    for pair in lines:\n",
        "        clean_pair = list()\n",
        "        for line in pair:\n",
        "            # normalize unicode characters\n",
        "            line = normalize('NFD', line).encode('ascii', 'ignore')\n",
        "            line = line.decode('UTF-8')\n",
        "            # tokenize on white space\n",
        "            line = line.split()\n",
        "            # convert to lowercase\n",
        "            line = [word.lower() for word in line]\n",
        "            # remove punctuation from each token\n",
        "            line = [word.translate(table) for word in line]\n",
        "            # remove non-printable chars form each token\n",
        "            line = [re_print.sub('', w) for w in line]\n",
        "            # remove tokens with numbers in them\n",
        "            line = [word for word in line if word.isalpha()]\n",
        "            # store as string\n",
        "            clean_pair.append(' '.join(line))\n",
        "        cleaned.append(clean_pair)\n",
        "    return numpy.array(cleaned)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgL2w0vYp_b_"
      },
      "source": [
        "#### Fill the following blanks:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "6Upni2sop_b_"
      },
      "outputs": [],
      "source": [
        "# e.g., filename = 'Data/deu.txt'\n",
        "filename = '/content/sample_data/spa.txt'\n",
        "# filename = '/content/sample_data/deu.txt'\n",
        "\n",
        "# e.g., n_train = 20000\n",
        "n_train = 20000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_G5mayRze6JZ"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "ywwHw8jHp_b_"
      },
      "outputs": [],
      "source": [
        "# load dataset\n",
        "doc = load_doc(filename)\n",
        "\n",
        "# split into Language1-Language2 pairs\n",
        "pairs = to_pairs(doc)\n",
        "\n",
        "# clean sentences\n",
        "clean_pairs = clean_data(pairs)[0:n_train, :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "iY-u7U1x1E33",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64a18d18-1ff6-4ee1-8eb6-a65df00958e1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20000, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "# pairs\n",
        "clean_pairs.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "6A3mqi-Tp_cA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df592afc-4397-4079-c0db-99a4db40870c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[were young] => [somos jovenes]\n",
            "[weve eaten] => [hemos comido]\n",
            "[what a bore] => [que aburrimiento]\n",
            "[what a dope] => [que burro eres]\n",
            "[what a dope] => [que burro]\n",
            "[what a heel] => [que tipo tan arrastrado]\n",
            "[what a jerk] => [que pendejo]\n",
            "[what a jerk] => [que imbecil]\n",
            "[what a jerk] => [que cretino]\n",
            "[what a life] => [que vida]\n"
          ]
        }
      ],
      "source": [
        "for i in range(3000, 3010):\n",
        "    print('[' + clean_pairs[i, 0] + '] => [' + clean_pairs[i, 1] + ']')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "qujk3z5bp_cA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebb86f2a-f248-4f57-8c2c-211d6834f4bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of input_texts:  (20000,)\n",
            "Length of target_texts: (20000,)\n"
          ]
        }
      ],
      "source": [
        "input_texts = clean_pairs[:, 0]\n",
        "target_texts = ['\\t' + text + '\\n' for text in clean_pairs[:, 1]]\n",
        "\n",
        "print('Length of input_texts:  ' + str(input_texts.shape))\n",
        "print('Length of target_texts: ' + str(input_texts.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "nx8AON9tp_cA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e53e42e1-7d08-486e-fa0c-b24cd23c0b46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max length of input  sentences: 18\n",
            "max length of target sentences: 48\n"
          ]
        }
      ],
      "source": [
        "max_encoder_seq_length = max(len(line) for line in input_texts)\n",
        "max_decoder_seq_length = max(len(line) for line in target_texts)\n",
        "\n",
        "print('max length of input  sentences: %d' % (max_encoder_seq_length))\n",
        "print('max length of target sentences: %d' % (max_decoder_seq_length))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFBWxjtBp_cA"
      },
      "source": [
        "**Remark:** To this end, you have two lists of sentences: input_texts and target_texts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZmG0mJ5p_cA"
      },
      "source": [
        "## 2. Text processing\n",
        "\n",
        "### 2.1. Convert texts to sequences\n",
        "\n",
        "- Input: A list of $n$ sentences (with max length $t$).\n",
        "- It is represented by a $n\\times t$ matrix after the tokenization and zero-padding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "yavP0xyfp_cA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32b1924c-5cbf-4661-c2e3-40a36f23a101"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of encoder_input_seq: (20000, 18)\n",
            "shape of input_token_index: 27\n",
            "shape of decoder_input_seq: (20000, 48)\n",
            "shape of target_token_index: 29\n"
          ]
        }
      ],
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "# from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import pad_sequences\n",
        "\n",
        "# encode and pad sequences\n",
        "def text2sequences(max_len, lines):\n",
        "    tokenizer = Tokenizer(char_level=True, filters='')\n",
        "    tokenizer.fit_on_texts(lines)\n",
        "    seqs = tokenizer.texts_to_sequences(lines)\n",
        "    seqs_pad = pad_sequences(seqs, maxlen=max_len, padding='post')\n",
        "    return seqs_pad, tokenizer.word_index\n",
        "\n",
        "\n",
        "encoder_input_seq, input_token_index = text2sequences(max_encoder_seq_length, \n",
        "                                                      input_texts)\n",
        "decoder_input_seq, target_token_index = text2sequences(max_decoder_seq_length, \n",
        "                                                       target_texts)\n",
        "\n",
        "print('shape of encoder_input_seq: ' + str(encoder_input_seq.shape))\n",
        "print('shape of input_token_index: ' + str(len(input_token_index)))\n",
        "print('shape of decoder_input_seq: ' + str(decoder_input_seq.shape))\n",
        "print('shape of target_token_index: ' + str(len(target_token_index)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "QyXqeAa5p_cA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c7d12e9-e7d4-4f8a-d367-d39333755c93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_encoder_tokens: 28\n",
            "num_decoder_tokens: 30\n"
          ]
        }
      ],
      "source": [
        "num_encoder_tokens = len(input_token_index) + 1\n",
        "num_decoder_tokens = len(target_token_index) + 1\n",
        "\n",
        "print('num_encoder_tokens: ' + str(num_encoder_tokens))\n",
        "print('num_decoder_tokens: ' + str(num_decoder_tokens))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5c4QRWXZp_cB"
      },
      "source": [
        "**Remark:** To this end, the input language and target language texts are converted to 2 matrices. \n",
        "\n",
        "- Their number of rows are both n_train.\n",
        "- Their number of columns are respective max_encoder_seq_length and max_decoder_seq_length."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shi0IgExp_cB"
      },
      "source": [
        "The followings print a sentence and its representation as a sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "_bet5GpHp_cB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "21c63f56-f580-453f-e71d-33c56f8c587d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\tsali\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "target_texts[100]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "kAAuRQJOp_cB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf64fef0-2a51-40d0-c140-faef9408d19e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 6,  5,  4, 12, 11,  7,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
              "      dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "decoder_input_seq[100, :]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljH1KLBLp_cB"
      },
      "source": [
        "## 2.2. One-hot encode\n",
        "\n",
        "- Input: A list of $n$ sentences (with max length $t$).\n",
        "- It is represented by a $n\\times t$ matrix after the tokenization and zero-padding.\n",
        "- It is represented by a $n\\times t \\times v$ tensor ($t$ is the number of unique chars) after the one-hot encoding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "at7-jjhJp_cB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00eeceaa-d7c1-449b-f523-9a029246952e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(20000, 18, 28)\n",
            "(20000, 48, 30)\n"
          ]
        }
      ],
      "source": [
        "from keras.utils import to_categorical\n",
        "\n",
        "# one hot encode target sequence\n",
        "def onehot_encode(sequences, max_len, vocab_size):\n",
        "    n = len(sequences)\n",
        "    data = numpy.zeros((n, max_len, vocab_size))\n",
        "    for i in range(n):\n",
        "        data[i, :, :] = to_categorical(sequences[i], num_classes=vocab_size)\n",
        "    return data\n",
        "\n",
        "encoder_input_data = onehot_encode(encoder_input_seq, max_encoder_seq_length, num_encoder_tokens)\n",
        "decoder_input_data = onehot_encode(decoder_input_seq, max_decoder_seq_length, num_decoder_tokens)\n",
        "\n",
        "decoder_target_seq = numpy.zeros(decoder_input_seq.shape)\n",
        "decoder_target_seq[:, 0:-1] = decoder_input_seq[:, 1:]\n",
        "decoder_target_data = onehot_encode(decoder_target_seq, \n",
        "                                    max_decoder_seq_length, \n",
        "                                    num_decoder_tokens)\n",
        "\n",
        "print(encoder_input_data.shape)\n",
        "print(decoder_input_data.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PaNwzlWsp_cB"
      },
      "source": [
        "## 3. Build the networks (for training)\n",
        "\n",
        "- Build encoder, decoder, and connect the two modules to get \"model\". \n",
        "\n",
        "- Fit the model on the bilingual data to train the parameters in the encoder and decoder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBFCfjmRp_cC"
      },
      "source": [
        "### 3.1. Encoder network\n",
        "\n",
        "- Input:  one-hot encode of the input language\n",
        "\n",
        "- Return: \n",
        "\n",
        "    -- output (all the hidden states   $h_1, \\cdots , h_t$) are always discarded\n",
        "    \n",
        "    -- the final hidden state  $h_t$\n",
        "    \n",
        "    -- the final conveyor belt $c_t$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "9SHo1l2up_cC"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Input, LSTM, Bidirectional, Concatenate\n",
        "from keras.models import Model\n",
        "\n",
        "latent_dim = 256\n",
        "\n",
        "# inputs of the encoder network\n",
        "encoder_inputs = Input(shape=(None, num_encoder_tokens), \n",
        "                       name='encoder_inputs')\n",
        "\n",
        "# set the LSTM layer\n",
        "encoder_lstm = LSTM(latent_dim, return_state=True, \n",
        "                    dropout=0.5, name='encoder_lstm')\n",
        "_, state_h, state_c = encoder_lstm(encoder_inputs)\n",
        "\n",
        "# set the BI-LSTM layer\n",
        "encoder_bilstm = Bidirectional(LSTM(latent_dim, return_state=True, \n",
        "                                  dropout=0.5, name='encoder_lstm'))\n",
        "\n",
        "_, forward_h, forward_c, backward_h, backward_c = encoder_bilstm(encoder_inputs)\n",
        "\n",
        "state_h = Concatenate()([forward_h, backward_h])\n",
        "state_c = Concatenate()([forward_c, backward_c])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# build the encoder network model\n",
        "encoder_model = Model(inputs=encoder_inputs, \n",
        "                      outputs=[state_h, state_c],\n",
        "                      name='encoder')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujDxdTHap_cC"
      },
      "source": [
        "Print a summary and save the encoder network structure to \"./encoder.pdf\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "vAAvlovMp_cC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa9ef2c7-954e-492a-e0fe-68058b9de43a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"encoder\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " encoder_inputs (InputLayer)  [(None, None, 28)]       0         \n",
            "                                                                 \n",
            " encoder_lstm (LSTM)         [(None, 256),             291840    \n",
            "                              (None, 256),                       \n",
            "                              (None, 256)]                       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 291,840\n",
            "Trainable params: 291,840\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from IPython.display import SVG\n",
        "from keras.utils.vis_utils import model_to_dot, plot_model\n",
        "\n",
        "SVG(model_to_dot(encoder_model, show_shapes=False).create(prog='dot', format='svg'))\n",
        "\n",
        "plot_model(\n",
        "    model=encoder_model, show_shapes=False,\n",
        "    to_file='encoder.pdf'\n",
        ")\n",
        "\n",
        "encoder_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "as9gs6Adp_cC"
      },
      "source": [
        "### 3.2. Decoder network\n",
        "\n",
        "- Inputs:  \n",
        "\n",
        "    -- one-hot encode of the target language\n",
        "    \n",
        "    -- The initial hidden state $h_t$ \n",
        "    \n",
        "    -- The initial conveyor belt $c_t$ \n",
        "\n",
        "- Return: \n",
        "\n",
        "    -- output (all the hidden states) $h_1, \\cdots , h_t$\n",
        "\n",
        "    -- the final hidden state  $h_t$ (discarded in the training and used in the prediction)\n",
        "    \n",
        "    -- the final conveyor belt $c_t$ (discarded in the training and used in the prediction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "DZJqVEfmp_cC"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Input, LSTM, Dense\n",
        "from keras.models import Model\n",
        "\n",
        "# inputs of the decoder network\n",
        "decoder_input_h = Input(shape=(latent_dim,), name='decoder_input_h')\n",
        "decoder_input_c = Input(shape=(latent_dim,), name='decoder_input_c')\n",
        "decoder_input_x = Input(shape=(None, num_decoder_tokens), name='decoder_input_x')\n",
        "\n",
        "# set the LSTM layer\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, \n",
        "                    return_state=True, dropout=0.5, name='decoder_lstm')\n",
        "decoder_lstm_outputs, state_h, state_c = decoder_lstm(decoder_input_x, \n",
        "                                                      initial_state=[decoder_input_h, decoder_input_c])\n",
        "\n",
        "# set the dense layer\n",
        "decoder_dense = Dense(num_decoder_tokens, activation='softmax', name='decoder_dense')\n",
        "decoder_outputs = decoder_dense(decoder_lstm_outputs)\n",
        "\n",
        "# build the decoder network model\n",
        "decoder_model = Model(inputs=[decoder_input_x, decoder_input_h, decoder_input_c],\n",
        "                      outputs=[decoder_outputs, state_h, state_c],\n",
        "                      name='decoder')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oze2sxp6p_cC"
      },
      "source": [
        "Print a summary and save the encoder network structure to \"./decoder.pdf\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "CNTSDUGkp_cD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3aabf61b-4165-4e27-e4b3-239ba15dd10f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"decoder\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " decoder_input_x (InputLayer)   [(None, None, 30)]   0           []                               \n",
            "                                                                                                  \n",
            " decoder_input_h (InputLayer)   [(None, 256)]        0           []                               \n",
            "                                                                                                  \n",
            " decoder_input_c (InputLayer)   [(None, 256)]        0           []                               \n",
            "                                                                                                  \n",
            " decoder_lstm (LSTM)            [(None, None, 256),  293888      ['decoder_input_x[0][0]',        \n",
            "                                 (None, 256),                     'decoder_input_h[0][0]',        \n",
            "                                 (None, 256)]                     'decoder_input_c[0][0]']        \n",
            "                                                                                                  \n",
            " decoder_dense (Dense)          (None, None, 30)     7710        ['decoder_lstm[0][0]']           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 301,598\n",
            "Trainable params: 301,598\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from IPython.display import SVG\n",
        "from keras.utils.vis_utils import model_to_dot, plot_model\n",
        "\n",
        "SVG(model_to_dot(decoder_model, show_shapes=False).create(prog='dot', format='svg'))\n",
        "\n",
        "plot_model(\n",
        "    model=decoder_model, show_shapes=False,\n",
        "    to_file='decoder.pdf'\n",
        ")\n",
        "\n",
        "decoder_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95M3eU2dp_cD"
      },
      "source": [
        "### 3.3. Connect the encoder and decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "Svfq4lmop_cD"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# input layers\n",
        "encoder_input_x = Input(shape=(None, num_encoder_tokens), name='encoder_input_x')\n",
        "decoder_input_x = Input(shape=(None, num_decoder_tokens), name='decoder_input_x')\n",
        "\n",
        "# connect encoder to decoder\n",
        "encoder_final_states = encoder_model([encoder_input_x])\n",
        "\n",
        "decoder_lstm_output, _, _ = decoder_lstm(decoder_input_x, initial_state=encoder_final_states)\n",
        "decoder_pred = decoder_dense(decoder_lstm_output)\n",
        "\n",
        "model = Model(inputs=[encoder_input_x, decoder_input_x], \n",
        "              outputs=decoder_pred, \n",
        "              name='model_training')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_input_x"
      ],
      "metadata": {
        "id": "G002eyXMvQoG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad26b2cf-4387-47a8-ced0-7dfaccb916c4"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<KerasTensor: shape=(None, None, 30) dtype=float32 (created by layer 'decoder_input_x')>"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "JgXViErip_cD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "719f72e0-13dd-47b9-c263-308992f15629"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KerasTensor(type_spec=TensorSpec(shape=(None, 256), dtype=tf.float32, name=None), name='decoder_lstm/PartitionedCall:2', description=\"created by layer 'decoder_lstm'\")\n",
            "KerasTensor(type_spec=TensorSpec(shape=(None, 256), dtype=tf.float32, name='decoder_input_h'), name='decoder_input_h', description=\"created by layer 'decoder_input_h'\")\n"
          ]
        }
      ],
      "source": [
        "print(state_h)\n",
        "print(decoder_input_h)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "0CijdbD5p_cD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94316b8c-7a31-4f92-ca39-bf4ce3d96082"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_training\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " encoder_input_x (InputLayer)   [(None, None, 28)]   0           []                               \n",
            "                                                                                                  \n",
            " decoder_input_x (InputLayer)   [(None, None, 30)]   0           []                               \n",
            "                                                                                                  \n",
            " encoder (Functional)           [(None, 256),        291840      ['encoder_input_x[0][0]']        \n",
            "                                 (None, 256)]                                                     \n",
            "                                                                                                  \n",
            " decoder_lstm (LSTM)            [(None, None, 256),  293888      ['decoder_input_x[0][0]',        \n",
            "                                 (None, 256),                     'encoder[1][0]',                \n",
            "                                 (None, 256)]                     'encoder[1][1]']                \n",
            "                                                                                                  \n",
            " decoder_dense (Dense)          (None, None, 30)     7710        ['decoder_lstm[2][0]']           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 593,438\n",
            "Trainable params: 593,438\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from IPython.display import SVG\n",
        "from keras.utils.vis_utils import model_to_dot, plot_model\n",
        "\n",
        "SVG(model_to_dot(model, show_shapes=False).create(prog='dot', format='svg'))\n",
        "\n",
        "plot_model(\n",
        "    model=model, show_shapes=False,\n",
        "    to_file='model_training.pdf'\n",
        ")\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qve5u1Nzp_cD"
      },
      "source": [
        "### 3.5. Fit the model on the bilingual dataset\n",
        "\n",
        "- encoder_input_data: one-hot encode of the input language\n",
        "\n",
        "- decoder_input_data: one-hot encode of the input language\n",
        "\n",
        "- decoder_target_data: labels (left shift of decoder_input_data)\n",
        "\n",
        "- tune the hyper-parameters\n",
        "\n",
        "- stop when the validation loss stop decreasing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "vvBUXUjDp_cD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05bb0d5e-cf4e-4a2e-e2f4-7c5e1000948d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of encoder_input_data(20000, 18, 28)\n",
            "shape of decoder_input_data(20000, 48, 30)\n",
            "shape of decoder_target_data(20000, 48, 30)\n"
          ]
        }
      ],
      "source": [
        "print('shape of encoder_input_data' + str(encoder_input_data.shape))\n",
        "print('shape of decoder_input_data' + str(decoder_input_data.shape))\n",
        "print('shape of decoder_target_data' + str(decoder_target_data.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "hohFaP_Ip_cE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b933ef6d-a1a1-4f7d-f428-f33e3e1ed03e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "250/250 [==============================] - 93s 349ms/step - loss: 1.0845 - val_loss: 1.1457\n",
            "Epoch 2/50\n",
            "250/250 [==============================] - 84s 337ms/step - loss: 0.9012 - val_loss: 0.9876\n",
            "Epoch 3/50\n",
            "250/250 [==============================] - 84s 338ms/step - loss: 0.8469 - val_loss: 0.9082\n",
            "Epoch 4/50\n",
            "250/250 [==============================] - 87s 347ms/step - loss: 0.8310 - val_loss: 0.8828\n",
            "Epoch 5/50\n",
            "250/250 [==============================] - 88s 352ms/step - loss: 0.8186 - val_loss: 0.8561\n",
            "Epoch 6/50\n",
            "250/250 [==============================] - 82s 330ms/step - loss: 0.8083 - val_loss: 0.8429\n",
            "Epoch 7/50\n",
            "250/250 [==============================] - 84s 338ms/step - loss: 0.7994 - val_loss: 0.8284\n",
            "Epoch 8/50\n",
            "250/250 [==============================] - 87s 348ms/step - loss: 0.7906 - val_loss: 0.8172\n",
            "Epoch 9/50\n",
            "250/250 [==============================] - 83s 332ms/step - loss: 0.7806 - val_loss: 0.8036\n",
            "Epoch 10/50\n",
            "250/250 [==============================] - 87s 348ms/step - loss: 0.7738 - val_loss: 0.7940\n",
            "Epoch 11/50\n",
            "250/250 [==============================] - 84s 336ms/step - loss: 0.7662 - val_loss: 0.7782\n",
            "Epoch 12/50\n",
            "250/250 [==============================] - 86s 344ms/step - loss: 0.7594 - val_loss: 0.7734\n",
            "Epoch 13/50\n",
            "250/250 [==============================] - 82s 330ms/step - loss: 0.7528 - val_loss: 0.7580\n",
            "Epoch 14/50\n",
            "250/250 [==============================] - 90s 361ms/step - loss: 0.7463 - val_loss: 0.7519\n",
            "Epoch 15/50\n",
            "250/250 [==============================] - 81s 325ms/step - loss: 0.7423 - val_loss: 0.7385\n",
            "Epoch 16/50\n",
            "250/250 [==============================] - 83s 333ms/step - loss: 0.7366 - val_loss: 0.7346\n",
            "Epoch 17/50\n",
            "250/250 [==============================] - 82s 328ms/step - loss: 0.7327 - val_loss: 0.7301\n",
            "Epoch 18/50\n",
            "250/250 [==============================] - 87s 348ms/step - loss: 0.7280 - val_loss: 0.7203\n",
            "Epoch 19/50\n",
            "250/250 [==============================] - 82s 327ms/step - loss: 0.7210 - val_loss: 0.7092\n",
            "Epoch 20/50\n",
            "250/250 [==============================] - 83s 332ms/step - loss: 0.7192 - val_loss: 0.7061\n",
            "Epoch 21/50\n",
            "250/250 [==============================] - 83s 332ms/step - loss: 0.7140 - val_loss: 0.7017\n",
            "Epoch 22/50\n",
            "250/250 [==============================] - 84s 334ms/step - loss: 0.7108 - val_loss: 0.6947\n",
            "Epoch 23/50\n",
            "250/250 [==============================] - 87s 348ms/step - loss: 0.7081 - val_loss: 0.6904\n",
            "Epoch 24/50\n",
            "250/250 [==============================] - 86s 343ms/step - loss: 0.7026 - val_loss: 0.6865\n",
            "Epoch 25/50\n",
            "250/250 [==============================] - 87s 348ms/step - loss: 0.6997 - val_loss: 0.6825\n",
            "Epoch 26/50\n",
            "250/250 [==============================] - 82s 329ms/step - loss: 0.6961 - val_loss: 0.6754\n",
            "Epoch 27/50\n",
            "250/250 [==============================] - 83s 331ms/step - loss: 0.6933 - val_loss: 0.6797\n",
            "Epoch 28/50\n",
            "250/250 [==============================] - 86s 346ms/step - loss: 0.6903 - val_loss: 0.6688\n",
            "Epoch 29/50\n",
            "250/250 [==============================] - 83s 333ms/step - loss: 0.6873 - val_loss: 0.6687\n",
            "Epoch 30/50\n",
            "250/250 [==============================] - 82s 328ms/step - loss: 0.6843 - val_loss: 0.6678\n",
            "Epoch 31/50\n",
            "250/250 [==============================] - 88s 352ms/step - loss: 0.6808 - val_loss: 0.6651\n",
            "Epoch 32/50\n",
            "250/250 [==============================] - 82s 327ms/step - loss: 0.6789 - val_loss: 0.6551\n",
            "Epoch 33/50\n",
            "250/250 [==============================] - 84s 335ms/step - loss: 0.6759 - val_loss: 0.6564\n",
            "Epoch 34/50\n",
            "250/250 [==============================] - 83s 333ms/step - loss: 0.6737 - val_loss: 0.6475\n",
            "Epoch 35/50\n",
            "250/250 [==============================] - 82s 328ms/step - loss: 0.6709 - val_loss: 0.6455\n",
            "Epoch 36/50\n",
            "250/250 [==============================] - 83s 332ms/step - loss: 0.6683 - val_loss: 0.6446\n",
            "Epoch 37/50\n",
            "250/250 [==============================] - 82s 328ms/step - loss: 0.6647 - val_loss: 0.6401\n",
            "Epoch 38/50\n",
            "250/250 [==============================] - 84s 336ms/step - loss: 0.6620 - val_loss: 0.6370\n",
            "Epoch 39/50\n",
            "250/250 [==============================] - 81s 325ms/step - loss: 0.6617 - val_loss: 0.6383\n",
            "Epoch 40/50\n",
            "250/250 [==============================] - 83s 334ms/step - loss: 0.6600 - val_loss: 0.6332\n",
            "Epoch 41/50\n",
            "250/250 [==============================] - 86s 345ms/step - loss: 0.6584 - val_loss: 0.6310\n",
            "Epoch 42/50\n",
            "250/250 [==============================] - 83s 333ms/step - loss: 0.6555 - val_loss: 0.6332\n",
            "Epoch 43/50\n",
            "250/250 [==============================] - 82s 330ms/step - loss: 0.6542 - val_loss: 0.6271\n",
            "Epoch 44/50\n",
            "250/250 [==============================] - 85s 340ms/step - loss: 0.6512 - val_loss: 0.6235\n",
            "Epoch 45/50\n",
            "250/250 [==============================] - 84s 334ms/step - loss: 0.6476 - val_loss: 0.6241\n",
            "Epoch 46/50\n",
            "250/250 [==============================] - 83s 331ms/step - loss: 0.6468 - val_loss: 0.6193\n",
            "Epoch 47/50\n",
            "250/250 [==============================] - 88s 351ms/step - loss: 0.6457 - val_loss: 0.6225\n",
            "Epoch 48/50\n",
            "250/250 [==============================] - 86s 346ms/step - loss: 0.6430 - val_loss: 0.6128\n",
            "Epoch 49/50\n",
            "250/250 [==============================] - 84s 334ms/step - loss: 0.6410 - val_loss: 0.6147\n",
            "Epoch 50/50\n",
            "250/250 [==============================] - 82s 329ms/step - loss: 0.6397 - val_loss: 0.6117\n"
          ]
        }
      ],
      "source": [
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
        "\n",
        "model.fit([encoder_input_data, decoder_input_data],  # training data\n",
        "          decoder_target_data,                       # labels (left shift of the target sequences)\n",
        "          batch_size=64, epochs=50, validation_split=0.2)\n",
        "\n",
        "model.save('seq2seq.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEi3DYCdp_cE"
      },
      "source": [
        "## 4. Make predictions\n",
        "\n",
        "\n",
        "### 4.1. Translate English to XXX\n",
        "\n",
        "1. Encoder read a sentence (source language) and output its final states, $h_t$ and $c_t$.\n",
        "2. Take the [star] sign \"\\t\" and the final state $h_t$ and $c_t$ as input and run the decoder.\n",
        "3. Get the new states and predicted probability distribution.\n",
        "4. sample a char from the predicted probability distribution\n",
        "5. take the sampled char and the new states as input and repeat the process (stop if reach the [stop] sign \"\\n\")."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "4AHJbi55p_cE"
      },
      "outputs": [],
      "source": [
        "# Reverse-lookup token index to decode sequences back to something readable.\n",
        "reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
        "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())\n",
        "\n",
        "reverse_testtarget_char_index=dict((i, char) for char, i in target_token_index.items())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mbl1iEEXxEQp"
      },
      "outputs": [],
      "source": [
        "reverse_input_char_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "Jpkds02Kp_cE"
      },
      "outputs": [],
      "source": [
        "def decode_sequence(input_seq):\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "    target_seq = numpy.zeros((1, 1, num_decoder_tokens))\n",
        "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
        "\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "\n",
        "        # this line of code is greedy selection\n",
        "        # try to use multinomial sampling instead (with temperature)\n",
        "        sampled_token_index = numpy.argmax(output_tokens[0, -1, :])\n",
        "        \n",
        "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "        #sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "        sampled_char = reverse_testtarget_char_index[sampled_token_index]\n",
        "        \n",
        "        decoded_sentence += sampled_char\n",
        "\n",
        "        if (sampled_char == '\\n' or\n",
        "           len(decoded_sentence) > max_decoder_seq_length):\n",
        "            stop_condition = True\n",
        "\n",
        "        target_seq = numpy.zeros((1, 1, num_decoder_tokens))\n",
        "        target_seq[0, 0, sampled_token_index] = 1.\n",
        "\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return decoded_sentence\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "Fka23nsMp_cE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74f7a94f-3e5f-426e-c599-d5b7cf497b68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 477ms/step\n",
            "1/1 [==============================] - 0s 444ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "-\n",
            "English:        dont smoke\n",
            "Spanish (true):  no fumeis\n",
            "Spanish (pred):  no se llares\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "-\n",
            "English:        dont smoke\n",
            "Spanish (true):  no fumais\n",
            "Spanish (pred):  no se llares\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "-\n",
            "English:        dont speak\n",
            "Spanish (true):  no hables\n",
            "Spanish (pred):  no se llares\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "-\n",
            "English:        dont worry\n",
            "Spanish (true):  no te preocupes\n",
            "Spanish (pred):  no se preguntes\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "-\n",
            "English:        dont worry\n",
            "Spanish (true):  no os preocupeis\n",
            "Spanish (pred):  no se preguntes\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "-\n",
            "English:        dont worry\n",
            "Spanish (true):  no se preocupen\n",
            "Spanish (pred):  no se preguntes\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "-\n",
            "English:        finish this\n",
            "Spanish (true):  termine esto\n",
            "Spanish (pred):  se cono es mi tom\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "-\n",
            "English:        finish this\n",
            "Spanish (true):  termina esto\n",
            "Spanish (pred):  se cono es mi tom\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "-\n",
            "English:        finish this\n",
            "Spanish (true):  termina esto\n",
            "Spanish (pred):  se cono es mi tom\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "-\n",
            "English:        finish this\n",
            "Spanish (true):  terminen esto\n",
            "Spanish (pred):  se cono es mi tom\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "-\n",
            "English:        finish this\n",
            "Spanish (true):  terminad esto\n",
            "Spanish (pred):  se cono es mi tom\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "-\n",
            "English:        flip a coin\n",
            "Spanish (true):  lanza una moneda\n",
            "Spanish (pred):  las a en casa\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "-\n",
            "English:        flip a coin\n",
            "Spanish (true):  tira una moneda al aire\n",
            "Spanish (pred):  las a en casa\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "-\n",
            "English:        forgive tom\n",
            "Spanish (true):  disculpe a tom\n",
            "Spanish (pred):  llama esto\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "-\n",
            "English:        forgive tom\n",
            "Spanish (true):  disculpa a tom\n",
            "Spanish (pred):  llama esto\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "-\n",
            "English:        get dressed\n",
            "Spanish (true):  vistete\n",
            "Spanish (pred):  abra esto\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "-\n",
            "English:        get dressed\n",
            "Spanish (true):  vestite\n",
            "Spanish (pred):  abra esto\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "-\n",
            "English:        get in here\n",
            "Spanish (true):  entra aqui\n",
            "Spanish (pred):  deja e tom\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "-\n",
            "English:        get serious\n",
            "Spanish (true):  ponte serio\n",
            "Spanish (pred):  ayudame a tom\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "-\n",
            "English:        get started\n",
            "Spanish (true):  empieza\n",
            "Spanish (pred):  dejame a tom\n"
          ]
        }
      ],
      "source": [
        "for seq_index in range(2100, 2120):\n",
        "    # Take one sequence (part of the training set)\n",
        "    # for trying out decoding.\n",
        "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
        "    decoded_sentence = decode_sequence(input_seq)\n",
        "    print('-')\n",
        "    print('English:       ', input_texts[seq_index])\n",
        "    print('Spanish (true): ', target_texts[seq_index][1:-1])\n",
        "    print('Spanish (pred): ', decoded_sentence[0:-1])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LoYi8EEp_cE"
      },
      "source": [
        "### 4.2. Translate an English sentence to the target language\n",
        "\n",
        "1. Tokenization\n",
        "2. One-hot encode\n",
        "3. Translate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "MpuihGkW4VKz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69b9c530-ee38-453f-eaf0-047367e6a06f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.preprocessing.text.Tokenizer at 0x7ff1b485fa00>"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ],
      "source": [
        "input_sentence = 'I love you'\n",
        "input_tokenizer = Tokenizer(num_words=30)\n",
        "input_tokenizer.fit_on_texts(input_sentence)\n",
        "input_integer_seq = input_tokenizer.texts_to_sequences(input_sentence)\n",
        "\n",
        "encoder_input_data[seq_index: seq_index + 1]\n",
        "# input_integer_seq\n",
        "input_tokenizer\n",
        "# states_value = encoder_model.predict(input_sentence)\n",
        "# states_value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "dz3nO1ODp_cE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97e796b6-7c81-4446-a1c3-d1349dc6ef99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 445ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "source sentence is: I love you\n",
            "translated sentence is: son esta es minto\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# example:\n",
        "# input_sentence = 'I love you'\n",
        "\n",
        "# input_sequence = <do tokenization...>\n",
        "# input_sequence = text2sequences(max_encoder_seq_length, input_sentence)[0]\n",
        "input_sequence = input_tokenizer.texts_to_sequences(input_sentence)[0]\n",
        "\n",
        "# input_x = <do one-hot encode...>\n",
        "input_x = onehot_encode(input_sequence, len(input_sentence), num_encoder_tokens)\n",
        "\n",
        "# translated_sentence = <do translation...>\n",
        "translated_sentence = decode_sequence(input_x)\n",
        "\n",
        "print('source sentence is: ' + input_sentence)\n",
        "print('translated sentence is: ' + translated_sentence)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}